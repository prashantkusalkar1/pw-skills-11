{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Ans: Ensemble Learning is a machine learning technique where multiple models (called base or weak learners) are combined to produce a single, stronger and more accurate model.\n",
        "\n",
        "A group of diverse models, when combined, can perform better than any individual model.\n",
        "\n",
        "Each model may make mistakes, but when their predictions are aggregated, errors cancel out, leading to better accuracy, stability, and generalization."
      ],
      "metadata": {
        "id": "mjsmW1qq4kCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans: Both Bagging and Boosting are ensemble learning techniques used to improve model performance, but they differ in how models are trained and combined.\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Trains multiple models independently\n",
        "\n",
        "Uses bootstrap samples (random sampling with replacement)\n",
        "\n",
        "Combines predictions by averaging or majority voting\n",
        "\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Trains models sequentially\n",
        "\n",
        "Each new model focuses more on previous errors\n",
        "\n",
        "Misclassified points receive higher weight"
      ],
      "metadata": {
        "id": "87jb1jad4z9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Bootstrap Sampling and its Role in Bagging (Random Forest)?\n",
        "\n",
        "Ans: Bootstrap sampling is a resampling technique where:\n",
        "\n",
        "Multiple datasets are created by randomly sampling from the original dataset with replacement\n",
        "\n",
        "Each bootstrap sample has the same size as the original dataset\n",
        "\n",
        "Some observations may appear multiple times, while others may be left out\n",
        "\n",
        "In Random Forest:\n",
        "\n",
        "Each tree is trained on a different bootstrap sample\n",
        "\n",
        "This introduces diversity among trees\n",
        "\n",
        "Diversity reduces variance and prevents overfitting"
      ],
      "metadata": {
        "id": "bBF7QzQe5DgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) Samples and OOB Score?\n",
        "\n",
        "Ans: Data points not selected in a particular bootstrap sample are called Out-of-Bag samples\n",
        "\n",
        "Approximately 36.8% of data is OOB for each tree\n",
        "\n",
        "Out-of-Bag samples are data points not included in a bootstrap sample. The OOB score evaluates model performance by testing each tree on its OOB data, providing an unbiased estimate of ensemble accuracy."
      ],
      "metadata": {
        "id": "NW8sLgWj5bYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ],
      "metadata": {
        "id": "39FnOGxI5ktD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance explains which input features contribute most to a model’s predictions. While both Decision Trees and Random Forests can compute feature importance, the method, stability, and reliability differ.\n",
        "\n",
        "Feature importance in a decision tree is based on impurity reduction within a single model and is unstable, while in a random forest it is averaged across multiple trees, making it more reliable and robust."
      ],
      "metadata": {
        "id": "eHEfyWbq5qHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores. (Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "2JnrVCHW7Ce2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Random Forest Feature Importance\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and select top 5\n",
        "top_5_features = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ").head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_5_features)# Question 6: Random Forest Feature Importance\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and select top 5\n",
        "top_5_features = feature_importance_df.sort_values(\n",
        "    by='Importance', ascending=False\n",
        ").head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "id": "ZxHObDEM7SRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "_Q0jkrvh7Y-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Bagging vs Single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)"
      ],
      "metadata": {
        "id": "v_j0160B7hN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "cO9BYDPk7k3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Random Forest Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "W8VpyHN87qZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "pKCZj4E-70XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Bagging Regressor vs Random Forest Regressor\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)"
      ],
      "metadata": {
        "id": "So0-qExy7476"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "PxmRo4hc77Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. Choosing Between Bagging and Boosting\n",
        "Decision Criteria\n",
        "Factor\t     Bagging\t         Boosting\n",
        "Main goal\t   Reduce variance\tReduce bias\n",
        "Data noise\t Works well\t    Sensitive\n",
        "Overfitting  risk\t          Lower\tHigher\n",
        "Complexity\t Lower\t         Higher\n",
        "\n",
        "\n",
        "2. Handling Overfitting\n",
        "Techniques Used\n",
        "\n",
        "Data-level controls\n",
        "\n",
        "Remove leakage features (e.g., post-loan information)\n",
        "\n",
        "Handle missing values carefully\n",
        "\n",
        "Feature scaling (if needed)\n",
        "\n",
        "Model-level controls\n",
        "\n",
        "Limit tree depth (max_depth)\n",
        "\n",
        "Use minimum samples per leaf\n",
        "\n",
        "Early stopping (for boosting)\n",
        "\n",
        "Feature subsampling (Random Forest)\n",
        "\n",
        "Validation-based controls\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Monitor train vs validation performance gap\n",
        "\n",
        "\n",
        "3. Selecting Base Models\n",
        "Preferred Base Learners\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Handle nonlinear relationships\n",
        "\n",
        "Interpretability (important in finance)\n",
        "\n",
        "Robust to outliers\n",
        "\n",
        "\n",
        "\n",
        "4. Performance Evaluation Using Cross-Validation\n",
        "Approach\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation\n",
        "\n",
        "Ensures default vs non-default ratio is preserved\n",
        "\n",
        "Evaluation Metrics\n",
        "\n",
        "Accuracy alone is insufficient. I use:\n",
        "\n",
        "ROC-AUC → overall discrimination power\n",
        "\n",
        "Precision-Recall → important for default class\n",
        "\n",
        "F1-Score → balance between precision & recall\n",
        "\n",
        "Confusion Matrix → business impact visibility\n",
        "\n",
        "Business Benefits\n",
        "\n",
        "Higher Predictive Accuracy\n",
        "\n",
        "Combines multiple weak models into a strong one\n",
        "\n",
        "Reduced Risk\n",
        "\n",
        "Lower variance → consistent loan decisions\n",
        "\n",
        "Fewer extreme or unstable predictions\n",
        "\n",
        "Better Customer Segmentation\n",
        "\n",
        "Captures complex spending and repayment behavior\n",
        "\n",
        "Explainability\n",
        "\n",
        "Feature importance helps justify decisions\n",
        "\n",
        "Required for compliance and audits\n",
        "\n",
        "Scalability\n",
        "\n",
        "Models adapt well to growing transaction data"
      ],
      "metadata": {
        "id": "vPABOMso8Fv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}